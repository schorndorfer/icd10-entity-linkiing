{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from rich.console import Console\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "cons = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "cons.print(f\"CUDA available: {cuda_available}\")\n",
    "if cuda_available:\n",
    "    cons.print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb90a5",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435538a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Get token from environment variable instead of hardcoding\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"Warning: HF_TOKEN not found in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d111ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"schorndorfer/mdace-inpatient\")\n",
    "cons.print(f\"Dataset loaded\")\n",
    "test_df = pl.from_dataframe(ds['test'].to_pandas())\n",
    "cons.print(f\"Test DataFrame shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cb472",
   "metadata": {},
   "source": [
    "## Load MedGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade --quiet accelerate bitsandbytes transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "google_colab = \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\")\n",
    "\n",
    "model_variant = \"4b-it\"  # @param [\"4b-it\", \"27b-it\", \"27b-text-it\"]\n",
    "model_variant = \"27b-text-it\"  # @param [\"4b-it\", \"27b-it\", \"27b-text-it\"]\n",
    "model_id = f\"google/medgemma-{model_variant}\"\n",
    "\n",
    "use_quantization = True  # @param {type: \"boolean\"}\n",
    "\n",
    "# @markdown Set `is_thinking` to `True` to turn on thinking mode. **Note:** Thinking is supported for the 27B variants only.\n",
    "is_thinking = True  # @param {type: \"boolean\"}\n",
    "\n",
    "# If running a 27B variant in Google Colab, check if the runtime satisfies\n",
    "# memory requirements\n",
    "if \"27b\" in model_variant and google_colab:\n",
    "    if not (\"A100\" in torch.cuda.get_device_name(0) and use_quantization):\n",
    "        raise ValueError(\n",
    "            \"Runtime has insufficient memory to run a 27B variant. \"\n",
    "            \"Please select an A100 GPU and use 4-bit quantization.\"\n",
    "        )\n",
    "\n",
    "model_kwargs = dict(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "if use_quantization:\n",
    "    model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402838eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "if \"text\" in model_variant:\n",
    "    pipe = pipeline(\"text-generation\", model=model_id, model_kwargs=model_kwargs)\n",
    "else:\n",
    "    pipe = pipeline(\"image-text-to-text\", model=model_id, model_kwargs=model_kwargs)\n",
    "\n",
    "pipe.model.generation_config.do_sample = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c851c",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You are an expert clinical coder. From the following phrase extracted from a medical note, along with the entire note as context,\n",
    "identify the most relevant ICD10-CM diagnosis codes\n",
    "\n",
    "Instructions:\n",
    "- Include all potential relevant codes\n",
    "- Include a code only once\n",
    "\n",
    "Output format:\n",
    "- **Code**: <code>, **Description**: <description>\n",
    "\n",
    "Just output a list of ICD-10 codes and descriptions, in the format described above.\n",
    "\n",
    "The input format will be:\n",
    "\n",
    "Input phrase:\n",
    "<phrase to code>\n",
    "\n",
    "Full medical note context:\n",
    "\n",
    "<full medical note context>\n",
    "\n",
    "###end###\n",
    "\n",
    "Output should be in the following format:\n",
    "<code 1>, <description 1>\n",
    "<code 2>, <description 2>\n",
    "<code 3>, <description 3>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20320ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict = test_df.row(2, named=True)\n",
    "input_phrase = row_dict['covered_text']\n",
    "clinical_note = row_dict['text']\n",
    "prompt = f\"\"\"\n",
    "Input phrase:\n",
    "{input_phrase}\n",
    "\\n\\n\n",
    "Full medical note context:\n",
    "{clinical_note}\n",
    "\\n\\n\n",
    "###end###\n",
    "\"\"\"\n",
    "row_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afcafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "role_instruction = \"You are an expert clinical coder. From the following medical note, identify the most relevant ICD-10 codes\"\n",
    "\n",
    "if \"27b\" in model_variant and is_thinking:\n",
    "    system_instruction = f\"SYSTEM INSTRUCTION: think silently if needed. {role_instruction}\"\n",
    "    max_new_tokens = 1500\n",
    "else:\n",
    "    system_instruction = role_instruction\n",
    "    max_new_tokens = 500\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af760354",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(messages, max_new_tokens=max_new_tokens)\n",
    "response = output[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\\n\\n---\"))\n",
    "display(Markdown(f\"**[ MedGemma ]**\\n\\n{response}\\n\\n---\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icd10-entity-linker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
